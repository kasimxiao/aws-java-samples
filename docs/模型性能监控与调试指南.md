# SageMaker 模型性能监控与调试指南

## 概述

本文档详细说明如何在 Amazon SageMaker 训练过程中监控和调试模型性能，包括：
- CloudWatch 日志查看和分析
- TensorBoard 可视化集成
- Weights & Biases (wandb) 实验跟踪
- SageMaker Debugger 性能分析

---

## 一、CloudWatch 日志监控

### 1.1 训练脚本日志输出

在训练脚本中，所有输出到 `stdout` 和 `stderr` 的内容都会自动发送到 CloudWatch Logs。

#### 基本日志输出示例

```python
# training_script.py
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def train():
    logger.info("开始训练...")
    
    for epoch in range(num_epochs):
        # 训练逻辑
        train_loss = train_one_epoch()
        val_loss = validate()
        
        # 输出训练指标（会自动发送到 CloudWatch）
        logger.info(f"Epoch {epoch+1}/{num_epochs}")
        logger.info(f"Train Loss: {train_loss:.4f}")
        logger.info(f"Validation Loss: {val_loss:.4f}")
        
        # 使用 print 也会被捕获
        print(f"Learning Rate: {current_lr}")
```


### 1.2 在 CloudWatch 中查看训练日志

#### 日志位置

训练作业日志存储在：
- **日志组**: `/aws/sagemaker/TrainingJobs`
- **日志流**: `[training-job-name]/algo-[instance-number]-[epoch_timestamp]`

#### 使用 AWS Console 查看

1. 打开 CloudWatch 控制台
2. 导航到 **Logs** > **Log groups**
3. 选择 `/aws/sagemaker/TrainingJobs`
4. 找到对应的训练作业日志流
5. 使用过滤器搜索特定内容（如 "ERROR", "Loss", "Accuracy"）

#### 使用 Boto3 查询日志

```python
import boto3
from datetime import datetime

logs_client = boto3.client('logs', region_name='us-west-2')

def get_training_logs(training_job_name, filter_pattern=None, limit=100):
    """获取训练作业日志"""
    log_group = '/aws/sagemaker/TrainingJobs'
    
    # 列出该训练作业的所有日志流
    response = logs_client.describe_log_streams(
        logGroupName=log_group,
        logStreamNamePrefix=training_job_name,
        orderBy='LastEventTime',
        descending=True
    )
    
    all_logs = []
    for stream in response['logStreams']:
        log_stream_name = stream['logStreamName']
        
        # 获取日志事件
        if filter_pattern:
            events_response = logs_client.filter_log_events(
                logGroupName=log_group,
                logStreamNames=[log_stream_name],
                filterPattern=filter_pattern,
                limit=limit
            )
        else:
            events_response = logs_client.get_log_events(
                logGroupName=log_group,
                logStreamName=log_stream_name,
                startFromHead=True,
                limit=limit
            )
        
        for event in events_response['events']:
            timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)
            all_logs.append({
                'timestamp': timestamp,
                'message': event['message']
            })
    
    return sorted(all_logs, key=lambda x: x['timestamp'])

# 使用示例
# 获取所有日志
logs = get_training_logs('my-training-job-2024-01-01-12-00-00')
for log in logs:
    print(f"[{log['timestamp']}] {log['message']}")

# 只获取包含 "Loss" 的日志
loss_logs = get_training_logs('my-training-job-2024-01-01-12-00-00', filter_pattern='Loss')
```


### 1.3 使用 SageMaker Python SDK 实时查看日志

```python
from sagemaker.estimator import Estimator

# 方法 1: 训练时实时查看日志
estimator = Estimator(
    image_uri='my-training-image',
    role=role,
    instance_count=1,
    instance_type='ml.p3.2xlarge'
)

# fit() 方法默认会实时输出日志
estimator.fit({'training': 's3://my-bucket/data'}, wait=True, logs='All')

# 方法 2: 附加到现有训练作业并查看日志
estimator = Estimator.attach('my-training-job-2024-01-01-12-00-00')
estimator.logs()  # 阻塞式输出所有日志
```

### 1.4 自定义指标提取

通过正则表达式从日志中提取自定义指标，并在 CloudWatch 中可视化。

```python
from sagemaker.estimator import Estimator

estimator = Estimator(
    image_uri='my-training-image',
    role=role,
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    # 定义指标提取规则
    metric_definitions=[
        {'Name': 'train:loss', 'Regex': 'Train Loss: ([0-9\\.]+)'},
        {'Name': 'train:accuracy', 'Regex': 'Train Accuracy: ([0-9\\.]+)'},
        {'Name': 'validation:loss', 'Regex': 'Validation Loss: ([0-9\\.]+)'},
        {'Name': 'validation:accuracy', 'Regex': 'Validation Accuracy: ([0-9\\.]+)'},
        {'Name': 'learning_rate', 'Regex': 'Learning Rate: ([0-9\\.e-]+)'},
    ]
)
```

在训练脚本中输出匹配的格式：

```python
# training_script.py
print(f"Train Loss: {train_loss:.6f}")
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Validation Loss: {val_loss:.6f}")
print(f"Validation Accuracy: {val_acc:.4f}")
print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6e}")
```

查询自定义指标：

```python
import boto3
from datetime import datetime, timedelta

cloudwatch = boto3.client('cloudwatch', region_name='us-west-2')

end_time = datetime.utcnow()
start_time = end_time - timedelta(hours=2)

# 查询训练损失
response = cloudwatch.get_metric_statistics(
    Namespace='AWS/SageMaker',
    MetricName='train:loss',
    Dimensions=[
        {'Name': 'TrainingJobName', 'Value': 'my-training-job-2024-01-01-12-00-00'}
    ],
    StartTime=start_time,
    EndTime=end_time,
    Period=60,
    Statistics=['Average']
)

for datapoint in sorted(response['Datapoints'], key=lambda x: x['Timestamp']):
    print(f"{datapoint['Timestamp']}: {datapoint['Average']:.4f}")
```

---

## 二、TensorBoard 可视化

### 2.1 TensorBoard 概述

Amazon SageMaker 支持 TensorBoard 集成，可以可视化：
- 训练和验证指标（损失、准确率等）
- 模型图结构
- 权重和梯度分布
- 嵌入向量可视化
- 图像、音频等数据

支持的框架：
- PyTorch
- TensorFlow
- Hugging Face Transformers


### 2.2 在训练脚本中集成 TensorBoard

#### PyTorch 示例

```python
# training_script.py
import torch
from torch.utils.tensorboard import SummaryWriter
import os

# TensorBoard 日志目录（SageMaker 会自动同步到 S3）
LOG_DIR = "/opt/ml/output/tensorboard"

def train():
    # 创建 TensorBoard writer
    writer = SummaryWriter(log_dir=LOG_DIR)
    
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(num_epochs):
        # 训练循环
        model.train()
        train_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # 记录每个 batch 的损失
            global_step = epoch * len(train_loader) + batch_idx
            writer.add_scalar('Loss/train_batch', loss.item(), global_step)
        
        # 记录每个 epoch 的平均损失
        avg_train_loss = train_loss / len(train_loader)
        writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)
        
        # 验证
        model.eval()
        val_loss = 0
        correct = 0
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                val_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        accuracy = 100. * correct / len(val_loader.dataset)
        
        # 记录验证指标
        writer.add_scalar('Loss/validation', avg_val_loss, epoch)
        writer.add_scalar('Accuracy/validation', accuracy, epoch)
        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)
        
        # 记录模型权重和梯度分布
        for name, param in model.named_parameters():
            writer.add_histogram(f'Weights/{name}', param, epoch)
            if param.grad is not None:
                writer.add_histogram(f'Gradients/{name}', param.grad, epoch)
        
        print(f'Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Accuracy={accuracy:.2f}%')
    
    # 关闭 writer
    writer.close()

if __name__ == '__main__':
    train()
```

#### TensorFlow/Keras 示例

```python
# training_script.py
import tensorflow as tf
from tensorflow import keras
import os

# TensorBoard 日志目录
LOG_DIR = "/opt/ml/output/tensorboard"

def train():
    model = create_model()
    
    # 创建 TensorBoard 回调
    tensorboard_callback = keras.callbacks.TensorBoard(
        log_dir=LOG_DIR,
        histogram_freq=1,  # 每个 epoch 记录权重分布
        write_graph=True,  # 记录模型图
        write_images=True,  # 记录模型权重为图像
        update_freq='epoch',  # 更新频率
        profile_batch='10,20'  # 性能分析的 batch 范围
    )
    
    # 自定义回调记录额外指标
    class CustomMetricsCallback(keras.callbacks.Callback):
        def __init__(self, log_dir):
            super().__init__()
            self.writer = tf.summary.create_file_writer(log_dir)
        
        def on_epoch_end(self, epoch, logs=None):
            with self.writer.as_default():
                # 记录学习率
                lr = self.model.optimizer.learning_rate
                if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):
                    lr = lr(self.model.optimizer.iterations)
                tf.summary.scalar('learning_rate', lr, step=epoch)
    
    custom_callback = CustomMetricsCallback(LOG_DIR)
    
    # 训练模型
    model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=num_epochs,
        callbacks=[tensorboard_callback, custom_callback]
    )

if __name__ == '__main__':
    train()
```


### 2.3 配置 SageMaker Estimator 支持 TensorBoard

```python
from sagemaker.pytorch import PyTorch
from sagemaker.debugger import TensorBoardOutputConfig
import os

# 定义 TensorBoard 输出配置
LOG_DIR = "/opt/ml/output/tensorboard"

# S3 输出路径
s3_output_path = f"s3://my-bucket/sagemaker-output/tensorboard/{job_name}"

tensorboard_output_config = TensorBoardOutputConfig(
    s3_output_path=s3_output_path,
    container_local_output_path=LOG_DIR
)

# 创建 Estimator
estimator = PyTorch(
    entry_point='training_script.py',
    source_dir='src',
    role=role,
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    framework_version='2.0',
    py_version='py310',
    # 配置 TensorBoard 输出
    tensorboard_output_config=tensorboard_output_config,
    hyperparameters={
        'epochs': 10,
        'batch-size': 32,
        'learning-rate': 0.001
    }
)

# 启动训练
estimator.fit({'training': 's3://my-bucket/training-data'})
```

### 2.4 访问 TensorBoard 应用

#### 方法 1: 通过 SageMaker Studio

1. 在 SageMaker Studio 中打开 TensorBoard
2. 选择训练作业
3. TensorBoard 会自动加载 S3 中的数据

#### 方法 2: 使用 Python SDK 生成访问链接

```python
from sagemaker.interactive_apps import tensorboard

# 生成 TensorBoard 访问 URL
app = tensorboard.TensorBoardApp(region='us-west-2')

# 打开 TensorBoard（会在浏览器中打开）
app.get_app_url(
    training_job_name='my-training-job-2024-01-01-12-00-00',
    open_in_default_web_browser=True
)
```

#### 方法 3: 本地运行 TensorBoard

```bash
# 下载 TensorBoard 数据到本地
aws s3 sync s3://my-bucket/sagemaker-output/tensorboard/my-training-job/ ./tensorboard_logs/

# 启动 TensorBoard
tensorboard --logdir=./tensorboard_logs --port=6006

# 在浏览器中访问 http://localhost:6006
```

### 2.5 TensorBoard 高级功能

#### 比较多个训练作业

```python
# 训练脚本中为不同实验使用不同的子目录
writer = SummaryWriter(log_dir=f"{LOG_DIR}/experiment_1")

# 或在 Estimator 中配置
tensorboard_output_config = TensorBoardOutputConfig(
    s3_output_path=f"s3://my-bucket/tensorboard/experiment_1"
)
```

在 TensorBoard 中可以同时加载多个实验进行对比。

#### 记录图像数据

```python
from torch.utils.tensorboard import SummaryWriter
import torchvision

writer = SummaryWriter(log_dir=LOG_DIR)

# 记录图像网格
img_grid = torchvision.utils.make_grid(images)
writer.add_image('input_images', img_grid, epoch)

# 记录单张图像
writer.add_image('prediction', pred_image, epoch)
```

#### 记录模型图

```python
# PyTorch
writer.add_graph(model, input_tensor)

# TensorFlow 会自动记录（如果 write_graph=True）
```

---

## 三、Weights & Biases (wandb) 集成

### 3.1 wandb 概述

Weights & Biases 是一个强大的实验跟踪和可视化平台，提供：
- 实验跟踪和对比
- 超参数优化
- 模型版本管理
- 协作和报告功能
- 比 TensorBoard 更丰富的可视化选项


### 3.2 在 SageMaker 中配置 wandb

#### 步骤 1: 获取 wandb API Key

1. 注册 [wandb.ai](https://wandb.ai) 账号
2. 在设置中获取 API Key
3. 将 API Key 存储在 AWS Secrets Manager 或作为环境变量传递

#### 步骤 2: 在训练脚本中集成 wandb

```python
# training_script.py
import wandb
import torch
import os

def train():
    # 初始化 wandb
    # API Key 可以通过环境变量传递
    wandb.login(key=os.environ.get('WANDB_API_KEY'))
    
    # 初始化项目
    wandb.init(
        project="sagemaker-training",
        name="experiment-1",
        config={
            "learning_rate": 0.001,
            "epochs": 10,
            "batch_size": 32,
            "architecture": "ResNet50",
            "dataset": "ImageNet"
        }
    )
    
    # 也可以从命令行参数获取配置
    config = wandb.config
    
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    
    # 监控模型（可选）
    wandb.watch(model, log='all', log_freq=100)
    
    for epoch in range(config.epochs):
        # 训练循环
        model.train()
        train_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # 记录每个 batch 的指标
            if batch_idx % 10 == 0:
                wandb.log({
                    "batch_loss": loss.item(),
                    "batch": epoch * len(train_loader) + batch_idx
                })
        
        # 验证
        model.eval()
        val_loss = 0
        correct = 0
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                val_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        accuracy = 100. * correct / len(val_loader.dataset)
        
        # 记录 epoch 级别的指标
        wandb.log({
            "epoch": epoch,
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
            "accuracy": accuracy,
            "learning_rate": optimizer.param_groups[0]['lr']
        })
        
        # 记录图像示例（可选）
        if epoch % 5 == 0:
            wandb.log({
                "examples": [wandb.Image(img) for img in sample_images[:5]]
            })
        
        print(f'Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Accuracy={accuracy:.2f}%')
    
    # 保存模型到 wandb
    model_path = "/opt/ml/model/model.pth"
    torch.save(model.state_dict(), model_path)
    wandb.save(model_path)
    
    # 完成训练
    wandb.finish()

if __name__ == '__main__':
    train()
```

#### 步骤 3: 配置 SageMaker Estimator

```python
from sagemaker.pytorch import PyTorch

# 方法 1: 通过环境变量传递 API Key
estimator = PyTorch(
    entry_point='training_script.py',
    source_dir='src',
    role=role,
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    framework_version='2.0',
    py_version='py310',
    environment={
        'WANDB_API_KEY': 'your-api-key-here'  # 不推荐硬编码
    },
    hyperparameters={
        'epochs': 10,
        'batch-size': 32,
        'learning-rate': 0.001
    }
)

# 方法 2: 从 AWS Secrets Manager 获取（推荐）
import boto3
import json

def get_secret(secret_name, region_name='us-west-2'):
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)
    response = client.get_secret_value(SecretId=secret_name)
    return json.loads(response['SecretString'])

wandb_secret = get_secret('wandb-api-key')

estimator = PyTorch(
    entry_point='training_script.py',
    source_dir='src',
    role=role,
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    framework_version='2.0',
    py_version='py310',
    environment={
        'WANDB_API_KEY': wandb_secret['api_key']
    }
)

# 启动训练
estimator.fit({'training': 's3://my-bucket/training-data'})
```


### 3.3 wandb 高级功能

#### 超参数扫描（Hyperparameter Sweep）

```python
# sweep_config.yaml
program: training_script.py
method: bayes  # 或 grid, random
metric:
  name: val_loss
  goal: minimize
parameters:
  learning_rate:
    min: 0.0001
    max: 0.1
  batch_size:
    values: [16, 32, 64, 128]
  epochs:
    value: 10
```

```python
# training_script.py with sweep
import wandb

def train():
    # wandb.init() 会自动从 sweep 获取配置
    run = wandb.init()
    config = wandb.config
    
    # 使用 config 中的超参数训练
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    
    # ... 训练循环 ...
    
    wandb.log({"val_loss": val_loss})

# 在 SageMaker 外部启动 sweep
sweep_id = wandb.sweep(sweep_config, project="sagemaker-training")

# 在 SageMaker 训练脚本中
if __name__ == '__main__':
    # 如果提供了 sweep_id，使用 agent
    sweep_id = os.environ.get('WANDB_SWEEP_ID')
    if sweep_id:
        wandb.agent(sweep_id, function=train, count=1)
    else:
        train()
```

#### 记录系统指标

```python
import wandb

# 自动记录系统指标（CPU、GPU、内存等）
wandb.init(
    project="sagemaker-training",
    config=config,
    # 启用系统监控
    settings=wandb.Settings(
        _stats_sample_rate_seconds=10,
        _stats_samples_to_average=3
    )
)
```

#### 记录自定义图表

```python
import wandb
import matplotlib.pyplot as plt

# 记录混淆矩阵
wandb.log({
    "confusion_matrix": wandb.plot.confusion_matrix(
        probs=None,
        y_true=ground_truth,
        preds=predictions,
        class_names=class_names
    )
})

# 记录 ROC 曲线
wandb.log({
    "roc_curve": wandb.plot.roc_curve(
        y_true=ground_truth,
        y_probas=predicted_probas,
        labels=class_names
    )
})

# 记录自定义 matplotlib 图表
fig, ax = plt.subplots()
ax.plot(epochs, train_losses, label='Train')
ax.plot(epochs, val_losses, label='Validation')
ax.legend()
wandb.log({"loss_curves": wandb.Image(fig)})
plt.close()
```

#### 模型版本管理

```python
import wandb

# 保存模型到 wandb Artifacts
artifact = wandb.Artifact('model', type='model')
artifact.add_file('/opt/ml/model/model.pth')
artifact.add_file('/opt/ml/model/config.json')
wandb.log_artifact(artifact)

# 在推理时加载模型
run = wandb.init(project="sagemaker-inference")
artifact = run.use_artifact('model:latest')
artifact_dir = artifact.download()
```

### 3.4 wandb 与 TensorBoard 对比

| 特性 | TensorBoard | Weights & Biases |
|------|-------------|------------------|
| 实验跟踪 | ✓ | ✓✓ (更强大) |
| 超参数对比 | 基础 | 高级（表格、平行坐标图） |
| 协作功能 | ✗ | ✓ (团队共享、评论) |
| 模型版本管理 | ✗ | ✓ |
| 超参数优化 | ✗ | ✓ (Sweep) |
| 报告生成 | ✗ | ✓ |
| 托管服务 | 需自建 | 云端托管 |
| 成本 | 免费 | 免费/付费 |
| 离线使用 | ✓ | 有限 |

---

## 四、SageMaker Debugger 性能分析

### 4.1 Debugger 概述

SageMaker Debugger 提供：
- 实时监控训练指标
- 自动检测训练问题（梯度消失/爆炸、过拟合等）
- 性能分析（CPU/GPU 利用率、I/O 瓶颈）
- 生成详细的分析报告


### 4.2 启用 Debugger

```python
from sagemaker.pytorch import PyTorch
from sagemaker.debugger import (
    Rule,
    rule_configs,
    DebuggerHookConfig,
    CollectionConfig,
    ProfilerConfig,
    FrameworkProfile
)

# 配置 Debugger Hook（收集张量数据）
debugger_hook_config = DebuggerHookConfig(
    s3_output_path=f's3://my-bucket/debugger-output/',
    collection_configs=[
        CollectionConfig(
            name="weights",
            parameters={
                "save_interval": "100"  # 每 100 步保存一次
            }
        ),
        CollectionConfig(
            name="gradients",
            parameters={
                "save_interval": "100"
            }
        ),
        CollectionConfig(
            name="losses",
            parameters={
                "save_interval": "10"
            }
        ),
    ]
)

# 配置性能分析
profiler_config = ProfilerConfig(
    system_monitor_interval_millis=500,  # 系统监控间隔（毫秒）
    framework_profile_params=FrameworkProfile(
        start_step=5,  # 开始分析的步骤
        num_steps=10   # 分析的步骤数
    )
)

# 配置内置规则
rules = [
    Rule.sagemaker(rule_configs.vanishing_gradient()),
    Rule.sagemaker(rule_configs.exploding_tensor()),
    Rule.sagemaker(rule_configs.loss_not_decreasing()),
    Rule.sagemaker(rule_configs.overfit()),
    Rule.sagemaker(rule_configs.overtraining()),
    Rule.sagemaker(rule_configs.poor_weight_initialization()),
]

# 创建 Estimator
estimator = PyTorch(
    entry_point='training_script.py',
    source_dir='src',
    role=role,
    instance_count=1,
    instance_type='ml.p3.2xlarge',
    framework_version='2.0',
    py_version='py310',
    # 启用 Debugger
    debugger_hook_config=debugger_hook_config,
    profiler_config=profiler_config,
    rules=rules
)

# 启动训练
estimator.fit({'training': 's3://my-bucket/training-data'})
```

### 4.3 在训练脚本中使用 Debugger Hook

```python
# training_script.py
import smdebug.pytorch as smd
import torch

def train():
    # 创建 Debugger hook
    hook = smd.Hook.create_from_json_file()
    
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters())
    
    # 注册模型和优化器
    hook.register_hook(model)
    
    for epoch in range(num_epochs):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            # 设置当前模式
            hook.set_mode(smd.modes.TRAIN)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            
            # 保存损失
            hook.save_scalar("loss", loss.item(), sm_metric=True)
            
            loss.backward()
            optimizer.step()
        
        # 验证
        model.eval()
        hook.set_mode(smd.modes.EVAL)
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                val_loss = criterion(output, target)
                hook.save_scalar("val_loss", val_loss.item(), sm_metric=True)

if __name__ == '__main__':
    train()
```

### 4.4 查看 Debugger 规则评估结果

```python
import boto3

sagemaker_client = boto3.client('sagemaker', region_name='us-west-2')

# 获取训练作业的规则评估状态
response = sagemaker_client.describe_training_job(
    TrainingJobName='my-training-job-2024-01-01-12-00-00'
)

print("Debugger 规则评估结果:")
for rule in response['DebugRuleEvaluationStatuses']:
    print(f"规则: {rule['RuleConfigurationName']}")
    print(f"状态: {rule['RuleEvaluationStatus']}")
    if 'StatusDetails' in rule:
        print(f"详情: {rule['StatusDetails']}")
    print()
```

### 4.5 下载和分析 Debugger 报告

```python
from sagemaker.debugger import ProfilerReport

# 下载性能分析报告
profiler_report = ProfilerReport.from_training_job(
    training_job_name='my-training-job-2024-01-01-12-00-00',
    sagemaker_session=sagemaker_session
)

# 报告会下载到本地
report_path = profiler_report.download()
print(f"报告已下载到: {report_path}")

# 在浏览器中打开 HTML 报告
import webbrowser
webbrowser.open(f'file://{report_path}/profiler-report.html')
```

### 4.6 使用 SMDebug 分析张量数据

```python
from smdebug.trials import create_trial

# 创建 trial 对象
trial = create_trial('s3://my-bucket/debugger-output/my-training-job-2024-01-01-12-00-00')

# 获取所有可用的张量名称
print("可用的张量:")
print(trial.tensor_names())

# 获取特定张量的值
loss_tensor = trial.tensor('loss')
steps = loss_tensor.steps()
values = [loss_tensor.value(step) for step in steps]

# 绘制损失曲线
import matplotlib.pyplot as plt
plt.plot(steps, values)
plt.xlabel('Step')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()

# 分析梯度
gradient_tensors = trial.tensor_names(regex='.*gradient.*')
for tensor_name in gradient_tensors[:5]:  # 只看前 5 个
    tensor = trial.tensor(tensor_name)
    print(f"{tensor_name}: shape={tensor.shape(0)}")
```


---

## 五、综合监控方案对比

### 5.1 不同监控工具的适用场景

| 监控工具 | 适用场景 | 优势 | 劣势 |
|---------|---------|------|------|
| **CloudWatch Logs** | 基础日志查看、问题排查 | 原生集成、无需额外配置 | 可视化能力弱 |
| **CloudWatch Metrics** | 资源监控、告警 | 与 AWS 深度集成、可设置告警 | 自定义指标需要正则提取 |
| **TensorBoard** | 深度学习训练可视化 | 免费、离线可用、框架原生支持 | 需要手动配置、协作功能弱 |
| **Weights & Biases** | 实验管理、团队协作 | 功能强大、易用、云端托管 | 需要账号、部分功能收费 |
| **SageMaker Debugger** | 自动问题检测、性能分析 | 自动化、深度集成 | 学习曲线较陡 |

### 5.2 推荐的组合方案

#### 方案 1: 基础方案（免费）
- **CloudWatch Logs**: 日志查看和问题排查
- **CloudWatch Metrics**: 资源监控和自定义指标
- **TensorBoard**: 训练过程可视化

适合：个人项目、小团队、预算有限

#### 方案 2: 进阶方案
- **CloudWatch**: 基础监控和告警
- **Weights & Biases**: 实验跟踪和对比
- **SageMaker Debugger**: 自动问题检测

适合：中大型团队、需要实验管理和协作

#### 方案 3: 完整方案
- **CloudWatch**: 基础设施监控
- **TensorBoard**: 实时训练可视化
- **Weights & Biases**: 实验管理和报告
- **SageMaker Debugger**: 性能分析和问题检测

适合：企业级应用、复杂模型训练

### 5.3 完整示例：集成所有监控工具

```python
# training_script.py - 集成所有监控工具
import logging
import torch
from torch.utils.tensorboard import SummaryWriter
import wandb
import smdebug.pytorch as smd
import os

# 配置日志（CloudWatch）
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def train():
    # 1. 初始化 TensorBoard
    tensorboard_writer = SummaryWriter(log_dir="/opt/ml/output/tensorboard")
    
    # 2. 初始化 wandb
    wandb.login(key=os.environ.get('WANDB_API_KEY'))
    wandb.init(
        project="sagemaker-comprehensive-monitoring",
        config={
            "learning_rate": 0.001,
            "epochs": 10,
            "batch_size": 32
        }
    )
    
    # 3. 初始化 SageMaker Debugger
    hook = smd.Hook.create_from_json_file()
    
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # 注册模型到监控工具
    hook.register_hook(model)
    wandb.watch(model, log='all', log_freq=100)
    
    logger.info("开始训练...")
    
    for epoch in range(10):
        model.train()
        hook.set_mode(smd.modes.TRAIN)
        
        train_loss = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            global_step = epoch * len(train_loader) + batch_idx
            
            # 记录到所有监控工具
            if batch_idx % 10 == 0:
                # CloudWatch Logs
                logger.info(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
                
                # TensorBoard
                tensorboard_writer.add_scalar('Loss/train_batch', loss.item(), global_step)
                
                # wandb
                wandb.log({"batch_loss": loss.item(), "batch": global_step})
                
                # Debugger
                hook.save_scalar("batch_loss", loss.item(), sm_metric=True)
        
        # Epoch 级别的指标
        avg_train_loss = train_loss / len(train_loader)
        
        # 验证
        model.eval()
        hook.set_mode(smd.modes.EVAL)
        val_loss = 0
        correct = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                val_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        accuracy = 100. * correct / len(val_loader.dataset)
        
        # 记录 epoch 指标到所有工具
        # CloudWatch Logs（会被提取为 CloudWatch Metrics）
        logger.info(f"Epoch {epoch+1}/{10}")
        logger.info(f"Train Loss: {avg_train_loss:.6f}")
        logger.info(f"Validation Loss: {avg_val_loss:.6f}")
        logger.info(f"Validation Accuracy: {accuracy:.2f}")
        
        # TensorBoard
        tensorboard_writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)
        tensorboard_writer.add_scalar('Loss/validation', avg_val_loss, epoch)
        tensorboard_writer.add_scalar('Accuracy/validation', accuracy, epoch)
        
        # 记录权重分布
        for name, param in model.named_parameters():
            tensorboard_writer.add_histogram(f'Weights/{name}', param, epoch)
            if param.grad is not None:
                tensorboard_writer.add_histogram(f'Gradients/{name}', param.grad, epoch)
        
        # wandb
        wandb.log({
            "epoch": epoch,
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
            "accuracy": accuracy,
            "learning_rate": optimizer.param_groups[0]['lr']
        })
        
        # Debugger
        hook.save_scalar("train_loss", avg_train_loss, sm_metric=True)
        hook.save_scalar("val_loss", avg_val_loss, sm_metric=True)
        hook.save_scalar("accuracy", accuracy, sm_metric=True)
    
    # 保存模型
    model_path = "/opt/ml/model/model.pth"
    torch.save(model.state_dict(), model_path)
    
    # 保存到 wandb
    wandb.save(model_path)
    
    # 清理
    tensorboard_writer.close()
    wandb.finish()
    
    logger.info("训练完成！")

if __name__ == '__main__':
    train()
```

